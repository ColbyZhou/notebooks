{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla RNN #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** A simple RNN using standard backprop algorithm, implemented by numpy **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n",
      "last updated: 2018-02-11 \n",
      "\n",
      "CPython 3.5.4\n",
      "IPython 6.2.1\n",
      "\n",
      "numpy 1.13.3\n",
      "\n",
      "compiler   : MSC v.1900 64 bit (AMD64)\n",
      "system     : Windows\n",
      "release    : 7\n",
      "machine    : AMD64\n",
      "processor  : Intel64 Family 6 Model 61 Stepping 4, GenuineIntel\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "# code environment\n",
    "%load_ext watermark\n",
    "%watermark -p numpy -v -m -u -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Description ###\n",
    "A standard RNN can be mathematically represented by:\n",
    "\n",
    "$h(t) = tanh(Wh(t-1) + UX + bh)$\n",
    "\n",
    "$f(t) = Vh(t) + bf$\n",
    "\n",
    "$p(t) = softmax(f(t))$\n",
    "\n",
    "* Here, we use $tanh$ and $softmax$ for hidden and output layer respectively.\n",
    "\n",
    "* In the follwing, all notations **without special declaration**, represent time $t$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume, hidden_size as $H$, input_size & output_size as $K$,namely a $K$-class classfifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notations & Sizes ###\n",
    "parameters: $W: H \\times H \\quad U:H \\times K \\quad bh: H \\times 1 \\quad V: K \\times H \\quad bf: K \\times 1$\n",
    "\n",
    "intermediate variables $p: K \\times 1 \\quad  f: K \\times 1 \\quad h: H \\times 1$\n",
    "\n",
    "input: $X: K \\times 1$\n",
    "\n",
    "derivation of h: $dh: H \\times 1 $\n",
    "\n",
    "derivation of f: $df: K \\times 1 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Reduction ###\n",
    "We Use **cross-entropy** as loss function,and the label of $m$-th sample is $y_m$.\n",
    "\n",
    "* output layer $\\Longrightarrow$ hidden layer\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "&\\frac{\\partial L_m}{\\partial f_k} = p_k - I(y_m = k) \\overset{def}{=} df_k \\\\\n",
    "&\\frac{\\partial L_m}{\\partial bf_k} = \\frac{\\partial L_m}{\\partial f_k} \\cdot \\frac{\\partial f_k}{\\partial bf_k} = df_k \\\\\n",
    "&\\frac{\\partial L_m}{\\partial v_{ki}} = \\frac{\\partial L_m}{\\partial f_k} \\cdot \\frac{\\partial f_k}{\\partial v_{ki}} = df_k \\cdot h_i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Matrix formulation**:\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "&\\frac{\\partial L_m}{\\partial bf} = df ,\\\\\n",
    "&\\frac{\\partial L_m}{\\partial V} = df \\cdot h(t)^T \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "* hidden layer $\\Longrightarrow$ input layer\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L_m}{\\partial bh_i} &=\\frac{\\partial L_m}{\\partial h_i} \\cdot \\frac{\\partial h_i}{\\partial bh_i} \\\\\n",
    "&=\\sum_{j}^{K}(\\frac{\\partial L_m}{\\partial f_j} \\cdot \\frac{\\partial f_j}{\\partial h_i}) \\cdot \\frac{\\partial h_i}{\\partial bh_i} \\\\\n",
    "&=(1-h_i^2) \\cdot \\sum_{j}^{K}(\\frac{\\partial L_m}{\\partial f_j} \\cdot \\frac{\\partial h_i}{\\partial bh_i}) \\\\\n",
    "&=(1-h_i^2) \\cdot \\sum_{j}^{K}(df_j \\cdot v_{ji}) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Image Description：\n",
    "![image.png](img/dbh.png)\n",
    "\n",
    "Here, we could regard $L_m$ as a function of $f_1, f_2,...,f_K$, and $f_j$ as function of $h_i$,:\n",
    "\n",
    "$$L_m = F(f_1,f_2,...,f_K) $$\n",
    "\n",
    "$$f_1 = G_1(h_i) \\quad  f_2 = G_2(h_i) \\quad ... \\quad f_K = G_K(h_i)$$\n",
    "\n",
    "According to the chain rule, we can get the result above.\n",
    "\n",
    "Learn More about backprop: http://colah.github.io/posts/2015-08-Backprop\n",
    "\n",
    "Go on!\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L_m}{\\partial u_{ij}} &=\\frac{\\partial L_m}{\\partial h_i} \\cdot \\frac{\\partial h_i}{\\partial u_{ij}} \\\\\n",
    "&=\\sum_{j}^{K}(df_j \\cdot v_{ji}) \\cdot \\frac{\\partial h_i}{\\partial u_{ij}} \\\\\n",
    "&=X_j \\cdot (1 - h_i^2) \\cdot \\sum_{j}^{K}(df_j \\cdot v_{ji}) \\\\\n",
    "&=X_j \\cdot \\frac{\\partial L_m}{\\partial bh_i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "same as above:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L_m}{\\partial w_{ni}} = h(t-1)_i \\cdot \\frac{\\partial L_m}{\\partial bh_i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "**Notice!** Here we regard h(t-1) as a **Constant**, which actually is a function of $W$. If we unfold $h(t-1)$, we get BPTT(Back Progation Through Time) algorithm.\n",
    "\n",
    "**Matrix formulation**:\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "&\\frac{\\partial L_m}{\\partial bh} = [(1 - h(t))\\otimes h(t)]\\otimes (V^T \\cdot df) ,\\\\\n",
    "&\\frac{\\partial L_m}{\\partial U} = \\frac{\\partial L_m}{\\partial bh} \\cdot X^T,\\\\\n",
    "&\\frac{\\partial L_m}{\\partial W} = \\frac{\\partial L_m}{\\partial bh} \\cdot h(t-1)^T,\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Let's go for gradient of h(t-1), here we regard h(t-1) as a variable:\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L_m}{\\partial h(t-1)_i} &= \\sum_n^H(\\frac{\\partial L_m}{\\partial h_n} \\cdot \\frac{\\partial h_n}{\\partial h(t-1)_i}) \\\\\n",
    "&=\\sum_n^H[ (\\sum_{j}^{K}(\\frac{\\partial L_m}{\\partial f_j} \\cdot \\frac{\\partial f_j}{\\partial h_n})) \\cdot (1-h_i^2)\\cdot w_{ni} ] \\\\\n",
    "&=\\sum_n^H(w_{ni} \\cdot \\frac{\\partial L_m}{\\partial bh_n})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Matrix formulation:**\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L_m}{\\partial h(t-1)} = W^T \\cdot \\frac{\\partial L_m}{\\partial bh}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Image Description：\n",
    "![image.png](img/dh1.png)\n",
    "\n",
    "Tips: as the image shows, the indexs of derivation associated with the edge, are the variable index of the edge's head and tail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 3\n",
    "input_size = 4\n",
    "inputs = [2]\n",
    "targets = [3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hidden_size $\\Longrightarrow H$ \n",
    "\n",
    "input_size $\\Longrightarrow K$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we initialize model parameters using Gaussian distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wxh = np.random.randn(hidden_size, input_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Whf = np.random.randn(input_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "bf = np.zeros((input_size, 1)) # output bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wxh $\\Longrightarrow U$ \n",
    "\n",
    "Whh $\\Longrightarrow W$\n",
    "\n",
    "Whf $\\Longrightarrow V$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00498441,  0.01500635, -0.01202295, -0.00355362],\n",
       "       [-0.00267535,  0.00416772, -0.00698555, -0.00545377],\n",
       "       [-0.00705934, -0.01143761, -0.0090071 , -0.00409386]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wxh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0074661 , -0.00860082,  0.00430556],\n",
       "       [ 0.01304914, -0.00546712,  0.02150136],\n",
       "       [-0.0096824 , -0.0068539 ,  0.01933451]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Whh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00649287,  0.00182899,  0.00344997],\n",
       "       [ 0.00669601,  0.00148885, -0.00129829],\n",
       "       [-0.00653117,  0.01307569, -0.00403534],\n",
       "       [-0.00280876, -0.01465321, -0.0089798 ]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Whf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Varaibles Dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, hs, fs, ps = {}, {}, {}, {} # all vaues,key: time t, value: vectors of time t\n",
    "hprev = np.zeros((hidden_size,1)) # previous hidden layer output\n",
    "hs[-1] = np.copy(hprev) # last hidhen layer output\n",
    "loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0 # example time t = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 0.]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs[t] = np.zeros((input_size, 1))\n",
    "xs[t][inputs[t]] = 1\n",
    "xs[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01202237],\n",
       "       [-0.00698543],\n",
       "       [-0.00900686]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh)\n",
    "hs[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3.42101089e-05],\n",
       "       [ -7.92087153e-05],\n",
       "       [  2.35265595e-05],\n",
       "       [  2.17006700e-04]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs[t] = np.dot(Whf, hs[t]) + bf\n",
    "fs[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.24999633],\n",
       "       [ 0.24996798],\n",
       "       [ 0.24999366],\n",
       "       [ 0.25004203]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps[t] = np.exp(fs[t]) / np.sum(np.exp(fs[t]))\n",
    "ps[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.250042032869\n",
      "1.38612624377\n"
     ]
    }
   ],
   "source": [
    "print(ps[t][targets[t], 0])\n",
    "loss += -np.log(ps[t][targets[t], 0])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dWxh, dWhh, dWhf = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Whf)\n",
    "dbh, dbf = np.zeros_like(bh), np.zeros_like(bf)\n",
    "dhnext = np.zeros_like(hs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.24999633],\n",
       "       [ 0.24996798],\n",
       "       [ 0.24999366],\n",
       "       [-0.74995797]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = np.copy(ps[t])\n",
    "df[targets[t]] -= 1\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00300555, -0.00174633, -0.00225168],\n",
       "       [-0.00300521, -0.00174613, -0.00225143],\n",
       "       [-0.00300552, -0.00174631, -0.00225166],\n",
       "       [ 0.00901627,  0.00523878,  0.00675476]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dWhf += np.dot(df, hs[t].T)\n",
    "dWhf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.24999633],\n",
       "       [ 0.24996798],\n",
       "       [ 0.24999366],\n",
       "       [-0.74995797]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbf += df\n",
    "dbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00052429],\n",
       "       [ 0.01508754],\n",
       "       [ 0.00626361]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dh = np.dot(Whf.T, df)\n",
    "dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01202237],\n",
       "       [-0.00698543],\n",
       "       [-0.00900686]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.99985546],\n",
       "       [ 0.9999512 ],\n",
       "       [ 0.99991888]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - hs[t] * hs[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00052422]\n",
      " [ 0.0150868 ]\n",
      " [ 0.0062631 ]]\n",
      "[[ 0.00052422]\n",
      " [ 0.0150868 ]\n",
      " [ 0.0062631 ]]\n",
      "[[ 0.00013231]\n",
      " [-0.00012992]\n",
      " [ 0.00044774]]\n"
     ]
    }
   ],
   "source": [
    "dhraw = (1 - hs[t] * hs[t]) * dh\n",
    "print(dhraw)\n",
    "dbh += dhraw\n",
    "print(dbh)\n",
    "dhnext = np.dot(Whh.T, dhraw)\n",
    "print(dhnext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.00052422  0.        ]\n",
      " [ 0.          0.          0.0150868   0.        ]\n",
      " [ 0.          0.          0.0062631   0.        ]]\n",
      "[[ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "dWxh += np.dot(dhraw, xs[t].T)\n",
    "print(dWxh)\n",
    "dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "print(dWhh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Reference ##\n",
    "1. http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "2. https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "3. http://cs231n.github.io/neural-networks-case-study/#grad\n",
    "4. https://www.zhihu.com/question/27239198?rf=24827633\n",
    "5. http://colah.github.io/posts/2015-08-Backprop/\n",
    "6. http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
