{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla RNN #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** A simple RNN using standard backprop algorithm, implemented by numpy **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n",
      "last updated: 2018-02-11 \n",
      "\n",
      "CPython 3.5.4\n",
      "IPython 6.2.1\n",
      "\n",
      "numpy 1.13.3\n",
      "\n",
      "compiler   : MSC v.1900 64 bit (AMD64)\n",
      "system     : Windows\n",
      "release    : 7\n",
      "machine    : AMD64\n",
      "processor  : Intel64 Family 6 Model 61 Stepping 4, GenuineIntel\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "# code environment\n",
    "%load_ext watermark\n",
    "%watermark -p numpy -v -m -u -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Description ###\n",
    "A standard RNN can be mathematically represented by:\n",
    "\n",
    "$h(t) = tanh(Wh(t-1) + UX + bh)$\n",
    "\n",
    "$f(t) = Vh(t) + bf$\n",
    "\n",
    "$p(t) = softmax(f(t))$\n",
    "\n",
    "* Here, we use $tanh$ and $softmax$ for hidden and output layer respectively.\n",
    "\n",
    "* In the follwing, all notations **without special declaration**, represent time $t$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume, hidden_size as $H$, input_size & output_size as $K$,namely a $K$-class classfifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notations & Sizes ###\n",
    "parameters: $W: H \\times H \\quad U:H \\times K \\quad bh: H \\times 1 \\quad V: K \\times H \\quad bf: K \\times 1$\n",
    "\n",
    "intermediate variables $p: K \\times 1 \\quad  f: K \\times 1 \\quad h: H \\times 1$\n",
    "\n",
    "input: $X: K \\times 1$\n",
    "\n",
    "derivation of h: $dh: H \\times 1 $\n",
    "\n",
    "derivation of f: $df: K \\times 1 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Reduction ###\n",
    "We Use **cross-entropy** as loss function,and the label of $m$-th sample is $y_m$.Then, the loss is:\n",
    "$$ L_m = -log(p_{y_m})$$\n",
    "Where,\n",
    "$$ p_k = \\frac{e^{f_k}}{\\sum_je^{f_j}} $$\n",
    "\n",
    "* output layer $\\Longrightarrow$ hidden layer\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "&\\frac{\\partial L_m}{\\partial f_k} = p_k - I(y_m = k) \\overset{def}{=} df_k \\\\\n",
    "&\\frac{\\partial L_m}{\\partial bf_k} = \\frac{\\partial L_m}{\\partial f_k} \\cdot \\frac{\\partial f_k}{\\partial bf_k} = df_k \\\\\n",
    "&\\frac{\\partial L_m}{\\partial v_{ki}} = \\frac{\\partial L_m}{\\partial f_k} \\cdot \\frac{\\partial f_k}{\\partial v_{ki}} = df_k \\cdot h_i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Matrix formulation**:\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "&\\frac{\\partial L_m}{\\partial bf} = df ,\\\\\n",
    "&\\frac{\\partial L_m}{\\partial V} = df \\cdot h(t)^T \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "* hidden layer $\\Longrightarrow$ input layer\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L_m}{\\partial bh_i} &=\\frac{\\partial L_m}{\\partial h_i} \\cdot \\frac{\\partial h_i}{\\partial bh_i} \\\\\n",
    "&=\\sum_{j}^{K}(\\frac{\\partial L_m}{\\partial f_j} \\cdot \\frac{\\partial f_j}{\\partial h_i}) \\cdot \\frac{\\partial h_i}{\\partial bh_i} \\\\\n",
    "&=(1-h_i^2) \\cdot \\sum_{j}^{K}(\\frac{\\partial L_m}{\\partial f_j} \\cdot \\frac{\\partial h_i}{\\partial bh_i}) \\\\\n",
    "&=(1-h_i^2) \\cdot \\sum_{j}^{K}(df_j \\cdot v_{ji}) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Image Description：\n",
    "![image.png](img/dbh.png)\n",
    "\n",
    "Here, we could regard $L_m$ as a function of $f_1, f_2,...,f_K$, and $f_j$ as function of $h_i$,:\n",
    "\n",
    "$$L_m = F(f_1,f_2,...,f_K) $$\n",
    "\n",
    "$$f_1 = G_1(h_i) \\quad  f_2 = G_2(h_i) \\quad ... \\quad f_K = G_K(h_i)$$\n",
    "\n",
    "According to the [chain rule](https://en.wikipedia.org/wiki/Chain_rule), we can get the result above.\n",
    "\n",
    "Learn More about backprop: http://colah.github.io/posts/2015-08-Backprop\n",
    "\n",
    "Go on!\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L_m}{\\partial u_{ij}} &=\\frac{\\partial L_m}{\\partial h_i} \\cdot \\frac{\\partial h_i}{\\partial u_{ij}} \\\\\n",
    "&=\\sum_{j}^{K}(df_j \\cdot v_{ji}) \\cdot \\frac{\\partial h_i}{\\partial u_{ij}} \\\\\n",
    "&=X_j \\cdot (1 - h_i^2) \\cdot \\sum_{j}^{K}(df_j \\cdot v_{ji}) \\\\\n",
    "&=X_j \\cdot \\frac{\\partial L_m}{\\partial bh_i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "same as above:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L_m}{\\partial w_{ni}} = h(t-1)_i \\cdot \\frac{\\partial L_m}{\\partial bh_i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "**Notice!** Here we assume h(t-1) as a **Constant**, which actually is a function of $W$. If we unfold $h(t-1)$, we get BPTT(Back Progation Through Time) algorithm.\n",
    "\n",
    "**Matrix formulation**:\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "&\\frac{\\partial L_m}{\\partial bh} = [1 - h(t)\\otimes h(t)]\\otimes (V^T \\cdot df) ,\\\\\n",
    "&\\frac{\\partial L_m}{\\partial U} = \\frac{\\partial L_m}{\\partial bh} \\cdot X^T,\\\\\n",
    "&\\frac{\\partial L_m}{\\partial W} = \\frac{\\partial L_m}{\\partial bh} \\cdot h(t-1)^T,\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Let's go for gradient of h(t-1), here we regard h(t-1) as a variable:\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L_m}{\\partial h(t-1)_i} &= \\sum_n^H(\\frac{\\partial L_m}{\\partial h_n} \\cdot \\frac{\\partial h_n}{\\partial h(t-1)_i}) \\\\\n",
    "&=\\sum_n^H[ (\\sum_{j}^{K}(\\frac{\\partial L_m}{\\partial f_j} \\cdot \\frac{\\partial f_j}{\\partial h_n})) \\cdot (1-h_i^2)\\cdot w_{ni} ] \\\\\n",
    "&=\\sum_n^H(w_{ni} \\cdot \\frac{\\partial L_m}{\\partial bh_n})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Matrix formulation:**\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L_m}{\\partial h(t-1)} = W^T \\cdot \\frac{\\partial L_m}{\\partial bh}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Image Description：\n",
    "![image.png](img/dh1.png)\n",
    "\n",
    "**Tips:** as the image shows, the indexs of derivation associated with the edge, are the variable index of the edge's head and tail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 3\n",
    "input_size = 4\n",
    "inputs = [2] # a sequence of input (word index)\n",
    "targets = [3] # a sequence of output (label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hidden_size $\\Longrightarrow H$ \n",
    "\n",
    "input_size $\\Longrightarrow K$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we initialize model parameters using Gaussian distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wxh = np.random.randn(hidden_size, input_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Whf = np.random.randn(input_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "bf = np.zeros((input_size, 1)) # output bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wxh $\\Longrightarrow U$ \n",
    "\n",
    "Whh $\\Longrightarrow W$\n",
    "\n",
    "Whf $\\Longrightarrow V$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4)\n",
      "[[ 0.00650017 -0.01253313 -0.01003872 -0.01774429]\n",
      " [-0.018472    0.02089544 -0.00193526  0.01330956]\n",
      " [ 0.01407877  0.00032981 -0.00234469  0.00779825]]\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(Wxh))\n",
    "print(Wxh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n",
      "[[-0.00237489 -0.00976991 -0.01425904]\n",
      " [-0.00313964 -0.00843796  0.01412579]\n",
      " [-0.00247709 -0.00382405  0.00385789]]\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(Whh))\n",
    "print(Whh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3)\n",
      "[[-0.01125553 -0.00075478 -0.00770129]\n",
      " [-0.00147965  0.00054511  0.00191957]\n",
      " [-0.01711854 -0.01223228  0.00162392]\n",
      " [-0.00239377 -0.02539663 -0.00834863]]\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(Whf))\n",
    "print(Whf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Varaibles Dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, hs, fs, ps = {}, {}, {}, {} # all vaues,key: time t, value: vectors of time t\n",
    "hprev = np.zeros((hidden_size,1)) # previous hidden layer output\n",
    "hs[-1] = np.copy(hprev) # last hidhen layer output\n",
    "loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Code, time **t = 0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0 # example time t = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one hot represent of input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]]\n"
     ]
    }
   ],
   "source": [
    "xs[t] = np.zeros((input_size, 1))\n",
    "xs[t][inputs[t]] = 1\n",
    "print(xs[t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**forword propagation**\n",
    "\n",
    "input layer $\\Longrightarrow$  hidden layer\n",
    "\n",
    "$h(t) = tanh(Wh(t-1) + UX + bh)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01313901],\n",
       "       [ 0.01292836],\n",
       "       [ 0.00354033]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs[t] = np.tanh(np.dot(Whh, hs[t-1]) + np.dot(Wxh, xs[t]) + bh)\n",
    "hs[t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hidden layer $\\Longrightarrow$  output layer\n",
    "\n",
    "\n",
    "$f(t) = Vh(t) + bf$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -1.28158120e-04],\n",
       "       [  4.18336099e-05],\n",
       "       [  3.53359438e-04],\n",
       "       [  1.79858915e-04]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs[t] = np.dot(Whf, hs[t]) + bf\n",
    "fs[t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(t) = softmax(f(t))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.24994003],\n",
       "       [ 0.24998252],\n",
       "       [ 0.25006041],\n",
       "       [ 0.25001703]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps[t] = np.exp(fs[t]) / np.sum(np.exp(fs[t]))\n",
    "ps[t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculus Loss**\n",
    "\n",
    "$ L_m = -log(p_{y_m})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.25001703]\n",
      "6.93113120674\n"
     ]
    }
   ],
   "source": [
    "print(ps[t][targets[t]])\n",
    "loss += -np.log(ps[t][targets[t], 0])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**backward propagation**\n",
    "\n",
    "Gradient Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dWxh, dWhh, dWhf = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Whf)\n",
    "dbh, dbf = np.zeros_like(bh), np.zeros_like(bf)\n",
    "dhnext = np.zeros_like(hs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial L_m}{\\partial f_k} = p_k - I(y_m = k) \\overset{def}{=} df_k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.24994003],\n",
       "       [ 0.24998252],\n",
       "       [ 0.25006041],\n",
       "       [-0.74998297]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = np.copy(ps[t])\n",
    "df[targets[t]] -= 1\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial L_m}{\\partial V} = df \\cdot h(t)^T $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00656793,  0.00646263,  0.00176974],\n",
       "       [ 0.00656905,  0.00646373,  0.00177004],\n",
       "       [ 0.00657109,  0.00646574,  0.0017706 ],\n",
       "       [-0.01970807, -0.01939209, -0.00531038]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dWhf += np.dot(df, hs[t].T)\n",
    "dWhf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial L_m}{\\partial bf} = df$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.24999633],\n",
       "       [ 0.24996798],\n",
       "       [ 0.24999366],\n",
       "       [-0.74995797]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbf += df\n",
    "dbf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial L_m}{\\partial bh} = [1 - h(t)\\otimes h(t)]\\otimes (V^T \\cdot df)$\n",
    "\n",
    "$\\frac{\\partial L_m}{\\partial h(t-1)} = W^T \\cdot \\frac{\\partial L_m}{\\partial bh}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00061993],\n",
       "       [-0.00476213],\n",
       "       [-0.00392948]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dh = np.dot(Whf.T, df) + dhnext # + dhnext, means back-prop twice at one time \n",
    "dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00061982]\n",
      " [-0.00476133]\n",
      " [-0.00392943]]\n",
      "[[ 0.00196492]\n",
      " [-0.01446199]\n",
      " [-0.01196527]]\n",
      "[[ -5.18042697e-05]\n",
      " [  8.74529206e-05]\n",
      " [  8.64490070e-05]]\n"
     ]
    }
   ],
   "source": [
    "dhraw = (1 - hs[t] * hs[t]) * dh\n",
    "print(dhraw)\n",
    "dbh += dhraw\n",
    "print(dbh)\n",
    "dhnext = np.dot(Whh.T, dhraw)\n",
    "print(dhnext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial L_m}{\\partial U} = \\frac{\\partial L_m}{\\partial bh} \\cdot X^T$\n",
    "\n",
    "$\\frac{\\partial L_m}{\\partial W} = \\frac{\\partial L_m}{\\partial bh} \\cdot h(t-1)^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.00129237  0.        ]\n",
      " [ 0.          0.         -0.00961166  0.        ]\n",
      " [ 0.          0.         -0.00794735  0.        ]]\n",
      "[[ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "dWxh += np.dot(dhraw, xs[t].T)\n",
    "print(dWxh)\n",
    "dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "print(dWhh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.93113120674\n",
      "[[ 0.          0.          0.00129237  0.        ]\n",
      " [ 0.          0.         -0.00961166  0.        ]\n",
      " [ 0.          0.         -0.00794735  0.        ]]\n",
      "[[ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "[[ 0.00656793  0.00646263  0.00176974]\n",
      " [ 0.00656905  0.00646373  0.00177004]\n",
      " [ 0.00657109  0.00646574  0.0017706 ]\n",
      " [-0.01970807 -0.01939209 -0.00531038]]\n",
      "[[ 0.00196492]\n",
      " [-0.01446199]\n",
      " [-0.01196527]]\n",
      "[[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "[[ 0.01313901]\n",
      " [ 0.01292836]\n",
      " [ 0.00354033]]\n"
     ]
    }
   ],
   "source": [
    "print(loss)\n",
    "print(dWxh)\n",
    "print(dWhh)\n",
    "print(dWhf)\n",
    "print(dbh)\n",
    "print(dbf)\n",
    "print(hs[len(inputs)-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Reference ##\n",
    "1. http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "2. https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "3. https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "4. http://cs231n.github.io/neural-networks-case-study/#grad\n",
    "5. https://www.zhihu.com/question/27239198?rf=24827633\n",
    "6. http://colah.github.io/posts/2015-08-Backprop/\n",
    "7. http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
