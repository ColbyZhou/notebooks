{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla RNN #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** A simple RNN using standard backprop algorithm, implemented by numpy **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n",
      "last updated: 2018-02-11 \n",
      "\n",
      "CPython 3.5.4\n",
      "IPython 6.2.1\n",
      "\n",
      "numpy 1.13.3\n",
      "\n",
      "compiler   : MSC v.1900 64 bit (AMD64)\n",
      "system     : Windows\n",
      "release    : 7\n",
      "machine    : AMD64\n",
      "processor  : Intel64 Family 6 Model 61 Stepping 4, GenuineIntel\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "# code environment\n",
    "%load_ext watermark\n",
    "%watermark -p numpy -v -m -u -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Description ###\n",
    "A standard RNN can be mathematically represented by:\n",
    "\n",
    "$h(t) = tanh(Wh(t-1) + UX + bh)$\n",
    "\n",
    "$f(t) = Vh(t) + bf$\n",
    "\n",
    "$p(t) = softmax(f(t))$\n",
    "\n",
    "* Here, we use $tanh$ and $softmax$ for hidden and output layer respectively.\n",
    "\n",
    "* In the follwing, all notations **without special declaration**, represent time $t$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume, hidden_size as $H$, input_size & output_size as $K$,namely a $K$-class classfifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notations & Sizes ###\n",
    "parameters: $W: H \\times H \\quad U:H \\times K \\quad bh: H \\times 1 \\quad V: K \\times H \\quad bf: K \\times 1$\n",
    "\n",
    "intermediate variables $p: K \\times 1 \\quad  f: K \\times 1 \\quad h: H \\times 1$\n",
    "\n",
    "input: $X: K \\times 1$\n",
    "\n",
    "derivation of h: $dh: H \\times 1 $\n",
    "\n",
    "derivation of f: $df: K \\times 1 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Reduction ###\n",
    "We Use **cross-entropy** as loss function,and the label of $m$-th sample is $y_m$.\n",
    "\n",
    "* output layer $\\Longrightarrow$ hidden layer\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "&\\frac{\\partial L_m}{\\partial f_k} = p_k - I(y_m = k) \\overset{def}{=} df_k \\\\\n",
    "&\\frac{\\partial L_m}{\\partial bf_k} = \\frac{\\partial L_m}{\\partial f_k} \\cdot \\frac{\\partial f_k}{\\partial bf_k} = df_k \\\\\n",
    "&\\frac{\\partial L_m}{\\partial v_{ki}} = \\frac{\\partial L_m}{\\partial f_k} \\cdot \\frac{\\partial f_k}{\\partial v_{ki}} = df_k \\cdot h_i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Matrix formulation**:\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "&\\frac{\\partial L_m}{\\partial bf} = df ,\\\\\n",
    "&\\frac{\\partial L_m}{\\partial V} = df \\cdot h(t)^T \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "* hidden layer $\\Longrightarrow$ input layer\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L_m}{\\partial bh_i} &=\\frac{\\partial L_m}{\\partial h_i} \\cdot \\frac{\\partial h_i}{\\partial bh_i} \\\\\n",
    "&=\\sum_{j}^{K}(\\frac{\\partial L_m}{\\partial f_j} \\cdot \\frac{\\partial f_j}{\\partial h_i}) \\cdot \\frac{\\partial h_i}{\\partial bh_i} \\\\\n",
    "&=(1-h_i^2) \\cdot \\sum_{j}^{K}(\\frac{\\partial L_m}{\\partial f_j} \\cdot \\frac{\\partial h_i}{\\partial bh_i}) \\\\\n",
    "&=(1-h_i^2) \\cdot \\sum_{j}^{K}(df_j \\cdot v_{ji}) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Image Description：\n",
    "![image.png](img/dbh.png)\n",
    "\n",
    "Here, we could regard $L_m$ as a function of $f_1, f_2,...,f_K$, and $f_j$ as function of $h_i$,:\n",
    "\n",
    "$$L_m = F(f_1,f_2,...,f_K) $$\n",
    "\n",
    "$$f_1 = G_1(h_i) \\quad  f_2 = G_2(h_i) \\quad ... \\quad f_K = G_K(h_i)$$\n",
    "\n",
    "According to the chain rule, we can get the result above.\n",
    "\n",
    "Learn More about backprop: http://colah.github.io/posts/2015-08-Backprop\n",
    "\n",
    "Go on!\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L_m}{\\partial u_{ij}} &=\\frac{\\partial L_m}{\\partial h_i} \\cdot \\frac{\\partial h_i}{\\partial u_{ij}} \\\\\n",
    "&=\\sum_{j}^{K}(df_j \\cdot v_{ji}) \\cdot \\frac{\\partial h_i}{\\partial u_{ij}} \\\\\n",
    "&=X_j \\cdot (1 - h_i^2) \\cdot \\sum_{j}^{K}(df_j \\cdot v_{ji}) \\\\\n",
    "&=X_j \\cdot \\frac{\\partial L_m}{\\partial bh_i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "same as above:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L_m}{\\partial w_{ni}} = h(t-1)_i \\cdot \\frac{\\partial L_m}{\\partial bh_i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "**Notice!** Here we regard h(t-1) as a **Constant**, which actually is a function of $W$. If we unfold $h(t-1)$, we get BPTT(Back Progation Through Time) algorithm.\n",
    "\n",
    "**Matrix formulation**:\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "&\\frac{\\partial L_m}{\\partial bh} = [(1 - h(t))\\otimes h(t)]\\otimes (V^T \\cdot df) ,\\\\\n",
    "&\\frac{\\partial L_m}{\\partial U} = \\frac{\\partial L_m}{\\partial bh} \\cdot X^T,\\\\\n",
    "&\\frac{\\partial L_m}{\\partial W} = \\frac{\\partial L_m}{\\partial bh} \\cdot h(t-1)^T,\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Let's go for gradient of h(t-1), here we regard h(t-1) as a variable:\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L_m}{\\partial h(t-1)_i} &= \\sum_n^H(\\frac{\\partial L_m}{\\partial h_n} \\cdot \\frac{\\partial h_n}{\\partial h(t-1)_i}) \\\\\n",
    "&=\\sum_n^H[ (\\sum_{j}^{K}(\\frac{\\partial L_m}{\\partial f_j} \\cdot \\frac{\\partial f_j}{\\partial h_n})) \\cdot (1-h_i^2)\\cdot w_{ni} ] \\\\\n",
    "&=\\sum_n^H(w_{ni} \\cdot \\frac{\\partial L_m}{\\partial bh_n})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Matrix formulation:**\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L_m}{\\partial h(t-1)} = W^T \\cdot \\frac{\\partial L_m}{\\partial bh}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Image Description：\n",
    "![image.png](img/dh1.png)\n",
    "\n",
    "Tips: as the image shows, the indexs of derivation associated with the edge, are the variable index of the edge's head and tail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 3\n",
    "vocab_size = 4\n",
    "inputs = [2]\n",
    "targets = [3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we initialize model parameters using Gaussian distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Whf = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "bf = np.zeros((vocab_size, 1)) # output bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01322516, -0.01297705, -0.01016583,  0.00896709],\n",
       "       [-0.00734309, -0.00930984,  0.01919849, -0.00213576],\n",
       "       [-0.00716901, -0.00230758,  0.01232394, -0.00354846]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wxh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00138893,  0.00209951,  0.00817658],\n",
       "       [ 0.00373148, -0.00764717, -0.00496181],\n",
       "       [ 0.00775307,  0.00230728, -0.01485889]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Whh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00036743,  0.01961332, -0.00445935],\n",
       "       [ 0.00347142,  0.01447653,  0.01592319],\n",
       "       [-0.00196813, -0.0012944 , -0.00408246],\n",
       "       [-0.01682938, -0.00894972, -0.0010431 ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Whf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, hs, fs, ps = {}, {}, {}, {}\n",
    "hprev = np.zeros((hidden_size,1))\n",
    "hs[-1] = np.copy(hprev)\n",
    "loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs[0] = np.zeros((vocab_size, 1))\n",
    "xs[0][inputs[0]] = 1\n",
    "xs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01016548],\n",
       "       [ 0.01919613],\n",
       "       [ 0.01232331]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs[0] = np.tanh(np.dot(Wxh, xs[0]) + np.dot(Whh, hs[0-1]) + bh)\n",
    "hs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3.17810854e-04],\n",
       "       [  4.38831154e-04],\n",
       "       [ -5.51498967e-05],\n",
       "       [ -1.35758787e-05]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs[0] = np.dot(Whf, hs[0]) + bf\n",
    "fs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.25003646],\n",
       "       [ 0.25006672],\n",
       "       [ 0.24994322],\n",
       "       [ 0.24995361]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps[0] = np.exp(fs[0]) / np.sum(np.exp(fs[0]))\n",
    "ps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.249953609992\n",
      "1.38647993837\n"
     ]
    }
   ],
   "source": [
    "print(ps[0][targets[0], 0])\n",
    "loss += -np.log(ps[0][targets[0], 0])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dWxh, dWhh, dWhf = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Whf)\n",
    "dbh, dbf = np.zeros_like(bh), np.zeros_like(bf)\n",
    "dhnext = np.zeros_like(hs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.25003646],\n",
       "       [ 0.25006672],\n",
       "       [ 0.24994322],\n",
       "       [-0.75004639]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = np.copy(ps[0])\n",
    "df[targets[0]] -= 1\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00254174,  0.00479973,  0.00308128],\n",
       "       [-0.00254205,  0.00480031,  0.00308165],\n",
       "       [-0.00254079,  0.00479794,  0.00308013],\n",
       "       [ 0.00762458, -0.01439799, -0.00924306]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dWhf += np.dot(df, hs[0].T)\n",
    "dWhf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.25003646],\n",
       "       [ 0.25006672],\n",
       "       [ 0.24994322],\n",
       "       [-0.75004639]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbf += df\n",
    "dbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01309085],\n",
       "       [ 0.01491332],\n",
       "       [ 0.00262885]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dh = np.dot(Whf.T, df)\n",
    "dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01016548],\n",
       "       [ 0.01919613],\n",
       "       [ 0.01232331]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.99989666],\n",
       "       [ 0.99963151],\n",
       "       [ 0.99984814]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - hs[0] * hs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0130895 ]\n",
      " [ 0.01490783]\n",
      " [ 0.00262845]]\n",
      "[[ 0.0130895 ]\n",
      " [ 0.01490783]\n",
      " [ 0.00262845]]\n",
      "[[  5.78264788e-05]\n",
      " [ -8.04564745e-05]\n",
      " [ -5.99831497e-06]]\n"
     ]
    }
   ],
   "source": [
    "dhraw = (1 - hs[0] * hs[0]) * dh\n",
    "print(dhraw)\n",
    "dbh += dhraw\n",
    "print(dbh)\n",
    "dhnext = np.dot(Whh.T, dhraw)\n",
    "print(dhnext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.0130895   0.        ]\n",
      " [ 0.          0.          0.01490783  0.        ]\n",
      " [ 0.          0.          0.00262845  0.        ]]\n",
      "[[ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "dWxh += np.dot(dhraw, xs[0].T)\n",
    "print(dWxh)\n",
    "dWhh += np.dot(dhraw, hs[0-1].T)\n",
    "print(dWhh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Reference ##\n",
    "1. http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "2. https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "3. http://cs231n.github.io/neural-networks-case-study/#grad\n",
    "4. https://www.zhihu.com/question/27239198?rf=24827633\n",
    "5. http://colah.github.io/posts/2015-08-Backprop/\n",
    "6. http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
